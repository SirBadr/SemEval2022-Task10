{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57439dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelToFile = {\n",
    "    'Threat':\"shapley_lexicon_1-aug.txt\",\n",
    "    'derogation':\"shapley_lexicon_2.txt\",\n",
    "    'animosity':\"shapley_lexicon_3.txt\",\n",
    "    'prejudiced discussions':\"shapley_lexicon_4.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e1dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_from_results(filename):\n",
    "    words = open('./Results/'+filename).read()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1382cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_tweet_for_augmentation(sentence, label):\n",
    "    words = read_file_from_results(labelToFile[label]); # threat\n",
    "    for word in sentence.split():\n",
    "        if word in words:\n",
    "            sentence = sentence.replace(word, \"[MASK]\")\n",
    "            print('found + ', sentence, 'word is ', word)\n",
    "            return sentence\n",
    "        else:\n",
    "            print('word not found!')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d478db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "target_column = \"label_category\"\n",
    "\n",
    "def read_tweets():\n",
    "    data = pd.read_csv(\"./Data/starting_ki/train_all_tasks.csv\")\n",
    "    data = data[data[target_column]!=\"none\"]\n",
    "    return data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70432ebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(read_file_from_results('shapley_lexicon_1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43a32f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word not found!\n",
      "word not found!\n",
      "found +  Threat: Husbands. [MASK] your piece of shit commie wives word is  kill\n",
      "Threat: Husbands. [MASK] your piece of shit commie wives\n"
     ]
    }
   ],
   "source": [
    "print(configure_tweet_for_augmentation('Threat: Husbands. kill your piece of shit commie wives', 'Threat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c24dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(read_tweets(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea574e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values = []\n",
    "if target_column == \"label_category\":\n",
    "  label_values = [\n",
    "      '1. threats, plans to harm and incitement',\n",
    "      '2. derogation',\n",
    "      '3. animosity',\n",
    "      '4. prejudiced discussions',\n",
    "  ]\n",
    "\n",
    "elif target_column == \"label_vector\":\n",
    "  label_values = [\n",
    "      '1.1 threats of harm',\n",
    "      '1.2 incitement and encouragement of harm',\n",
    "      '2.1 descriptive attacks',\n",
    "      '2.2 aggressive and emotive attacks',\n",
    "      '2.3 dehumanising attacks & overt sexual objectification',\n",
    "      '3.1 casual use of gendered slurs, profanities, and insults',\n",
    "      '3.2 immutable gender differences and gender stereotypes',\n",
    "      '3.3 backhanded gendered compliments',\n",
    "      '3.4 condescending explanations or unwelcome advice',\n",
    "      '4.1 supporting mistreatment of individual women',\n",
    "      '4.2 supporting systemic discrimination against women as a group',\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce464b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_fill_blank(text1, label1):\n",
    "    # define a function that takes as input two samples and generates the prompt\n",
    "    # that we should pass to the GPT-3 language model for completion.\n",
    "    prompt = (f\"{text1}. \\n\"\n",
    "             f\"Replace [MASK] with the most probable 2 {label1} words to replace.\")\n",
    "#     prompt = (f\"{description}\\n\"\n",
    "#             f\"Tweet: {text1} (Category: {label1})\\n\"\n",
    "#             f\"Tweet: {text2} (Category: {label2})\\n\"\n",
    "#             f\"Tweet:\")\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9d2af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Husbands. [MASK] your piece of shit commie wives. \n",
      "Replace [MASK] with the most probable 2 Threat words to replace.\n"
     ]
    }
   ],
   "source": [
    "print(get_prompt_fill_blank('Husbands. [MASK] your piece of shit commie wives','Threat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "864de60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions tensor([[-1.6710, -1.1899, -1.3171,  ...,  1.3490,  0.8899, -0.5004]])\n",
      "probs torch.Size([])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m         token_weight \u001b[38;5;241m=\u001b[39m top_k_weights[i]\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mpredicted_token, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(token_weight))\n\u001b[0;32m---> 49\u001b[0m \u001b[43mpredict_masked_sent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHusbands. [MASK] your piece of shit commie wives.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mThe above code will output:\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m[MASK]: 'mom'  | weights: 0.10288725048303604\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m[MASK]: 'sister'  | weights: 0.04804788902401924\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 43\u001b[0m, in \u001b[0;36mpredict_masked_sent\u001b[0;34m(text, top_k)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, probs\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     41\u001b[0m top_k_weights, top_k_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(probs, top_k, \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pred_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtop_k_indices\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     44\u001b[0m     predicted_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens([pred_idx])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     45\u001b[0m     token_weight \u001b[38;5;241m=\u001b[39m top_k_weights[i]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/SemEval22BackUp/lib/python3.10/site-packages/torch/_tensor.py:916\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 916\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    918\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    919\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    924\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    925\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)# OPTIONAL\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# print(\"tokenizer\", tokenizer)\n",
    "# out_dir = f'./Models/1a_fine-tuned-bert'\n",
    "# sexist_model = BertForSequenceClassification.from_pretrained(out_dir, num_labels=2)\n",
    "\n",
    "non_sexist_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "# sexist_model.classifier = non_sexist_model.cls\n",
    "non_sexist_model.eval()\n",
    "# model.to('cuda')  # if you have gpu\n",
    "\n",
    "# print(sexist_model.cls)\n",
    "\n",
    "def predict_masked_sent(text, top_k=5):\n",
    "    # Tokenize input\n",
    "    text = \"[CLS] %s [SEP]\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "#     print(\"tokenized_text\", tokenized_text)\n",
    "    masked_index = tokenized_text.index(\"[MASK]\")\n",
    "#     print(\"masked_index\", masked_index)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#     print(\"indexed_tokens\", indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = non_sexist_model(tokens_tensor)\n",
    "#         print(\"outputs\", outputs)\n",
    "        predictions = outputs[0]\n",
    "        print(\"predictions\", predictions)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1) # , dim=-1\n",
    "    print(\"probs\", probs.size())\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "\n",
    "        \n",
    "predict_masked_sent(\"Husbands. [MASK] your piece of shit commie wives.\", top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62780970",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254373bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f'./Models/1a_fine-tuned-bert'\n",
    "model = BertForSequenceClassification.from_pretrained(out_dir)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea43ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23375,"status":"ok","timestamp":1675721165919,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"DqQOwxhJ5EvF","outputId":"93511599-d2fa-4454-9c82-36c469cf3aae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PexO6fUd7kGP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":811,"status":"ok","timestamp":1675721166723,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"QK8O_gMj9g-l","outputId":"65338b15-bdcd-494f-887a-acd075dd436d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1lC-ZKLaCDQyfLcof2Ak7FDa6IvTt318A/SemEval2023/SemEval2022-Task10/re_run\n"]}],"source":["cd /content/gdrive/MyDrive/SemEval2023/SemEval2022-Task10/re_run"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2994,"status":"ok","timestamp":1675721169715,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"Ys8f8QuH9liv"},"outputs":[],"source":["import pandas as pd\n","\n","data = pd.read_csv(\"../Data/starting_ki/train_all_tasks.csv\")\n","# dfa = pd.read_csv(\"Data/dev_task_a_entries.csv\")\n","# dfb = pd.read_csv(\"Data/dev_task_b_entries.csv\")\n","# dfc = pd.read_csv(\"Data/dev_task_c_entries.csv\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1675721169716,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"9GeyhHIflAeI"},"outputs":[],"source":["import json\n","\n","# with open('../Results/TaskB/lexicon_pmi.json') as fin:\n","#     lexicon_pmi = json.load(fin)\n","\n","# with open('../Results/TaskB/lexicon_shapley.json') as fin:\n","#     lexicon_shapley = json.load(fin)\n","\n","# with open('../Results/TaskB/lexicon_shapley_augmented_v2_shap.json') as fin:\n","#     lexicon_shapley_augmented_v2_shap = json.load(fin)\n","\n","# with open('../Results/TaskB/lexicon_shapley_augmented_v3_gptj.json') as fin:\n","#     lexicon_shapley_augmented_v3_gptj = json.load(fin)\n","\n","# with open('../Results/TaskB/lexicon_shapley_augmented_v4_tweetbert.json') as fin:\n","#     lexicon_shapley_augmented_v4_tweetbert = json.load(fin)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1675721169716,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"P-ZQrn6gi1xL","outputId":"fab406ab-4630-4408-f732-99061f57a986"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                      rewire_id  \\\n","0       sexism2022_english-7358   \n","1       sexism2022_english-2367   \n","2       sexism2022_english-3073   \n","3      sexism2022_english-14895   \n","4       sexism2022_english-4118   \n","...                         ...   \n","13995  sexism2022_english-14185   \n","13996  sexism2022_english-18629   \n","13997   sexism2022_english-3320   \n","13998  sexism2022_english-13309   \n","13999  sexism2022_english-12402   \n","\n","                                                    text label_sexist  \\\n","0                  Damn, this writing was pretty chaotic   not sexist   \n","1      Yeah, and apparently a bunch of misogynistic v...   not sexist   \n","2           How the FUCK is this woman still an MP!!!???   not sexist   \n","3      Understand. Know you're right. At same time I ...   not sexist   \n","4         Surprized they didn't stop and rape some women   not sexist   \n","...                                                  ...          ...   \n","13995  complexes like the 'nice chicks' that go after...       sexist   \n","13996  \"GRAPHIC Germany - Muslim \"refugee\" stabbing h...   not sexist   \n","13997  Lol I imagine there would be simps that are li...   not sexist   \n","13998  It's not, the girls I go on dates with don't k...   not sexist   \n","13999  How can he be an incel? He does have a girlfri...   not sexist   \n","\n","      label_category             label_vector  \n","0               none                     none  \n","1               none                     none  \n","2               none                     none  \n","3               none                     none  \n","4               none                     none  \n","...              ...                      ...  \n","13995  2. derogation  2.1 descriptive attacks  \n","13996           none                     none  \n","13997           none                     none  \n","13998           none                     none  \n","13999           none                     none  \n","\n","[14000 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-1d564a92-5c93-4885-a147-01083445317f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rewire_id</th>\n","      <th>text</th>\n","      <th>label_sexist</th>\n","      <th>label_category</th>\n","      <th>label_vector</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>sexism2022_english-7358</td>\n","      <td>Damn, this writing was pretty chaotic</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sexism2022_english-2367</td>\n","      <td>Yeah, and apparently a bunch of misogynistic v...</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sexism2022_english-3073</td>\n","      <td>How the FUCK is this woman still an MP!!!???</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sexism2022_english-14895</td>\n","      <td>Understand. Know you're right. At same time I ...</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>sexism2022_english-4118</td>\n","      <td>Surprized they didn't stop and rape some women</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>13995</th>\n","      <td>sexism2022_english-14185</td>\n","      <td>complexes like the 'nice chicks' that go after...</td>\n","      <td>sexist</td>\n","      <td>2. derogation</td>\n","      <td>2.1 descriptive attacks</td>\n","    </tr>\n","    <tr>\n","      <th>13996</th>\n","      <td>sexism2022_english-18629</td>\n","      <td>\"GRAPHIC Germany - Muslim \"refugee\" stabbing h...</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>13997</th>\n","      <td>sexism2022_english-3320</td>\n","      <td>Lol I imagine there would be simps that are li...</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>13998</th>\n","      <td>sexism2022_english-13309</td>\n","      <td>It's not, the girls I go on dates with don't k...</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>13999</th>\n","      <td>sexism2022_english-12402</td>\n","      <td>How can he be an incel? He does have a girlfri...</td>\n","      <td>not sexist</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14000 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d564a92-5c93-4885-a147-01083445317f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1d564a92-5c93-4885-a147-01083445317f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1d564a92-5c93-4885-a147-01083445317f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["data"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1675721169716,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"_ohZZ4d6ik88"},"outputs":[],"source":["task = \"1c\""]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1675721169717,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"FXgFoL7_9nVS"},"outputs":[],"source":["if not task.endswith(\"a\"):\n","  data = data[data[\"label_category\"]!=\"none\"]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1675721169717,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"j66U-GOwatGG","outputId":"e7d82337-2695-4814-95bf-9ec8ae89fcdd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'label_vector'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["if task.endswith(\"a\"):\n","  label_column = \"label_sexist\"\n","elif task.endswith(\"b\"):\n","  label_column = \"label_category\"\n","elif task.endswith(\"c\"):\n","  label_column = \"label_vector\"\n","\n","\n","label_column"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1675721169717,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"ZS_RSluAiwbS","outputId":"eb87529e-256b-497c-9882-127c1b940bbc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3398"]},"metadata":{},"execution_count":10}],"source":["len(data)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1675721169718,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"zMV55B749pHq","outputId":"0a3821a4-f97a-4fa5-ec38-bbb584bad4a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    text\n","label_vector                                            \n","1.1 threats of harm                                   56\n","1.2 incitement and encouragement of harm             254\n","2.1 descriptive attacks                              717\n","2.2 aggressive and emotive attacks                   673\n","2.3 dehumanising attacks & overt sexual objecti...   200\n","3.1 casual use of gendered slurs, profanities, ...   637\n","3.2 immutable gender differences and gender ste...   417\n","3.3 backhanded gendered compliments                   64\n","3.4 condescending explanations or unwelcome advice    47\n","4.1 supporting mistreatment of individual women       75\n","4.2 supporting systemic discrimination against ...   258"],"text/html":["\n","  <div id=\"df-8dfe0780-00d8-41e4-8dfa-c4a900c2412f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","    <tr>\n","      <th>label_vector</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1.1 threats of harm</th>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>1.2 incitement and encouragement of harm</th>\n","      <td>254</td>\n","    </tr>\n","    <tr>\n","      <th>2.1 descriptive attacks</th>\n","      <td>717</td>\n","    </tr>\n","    <tr>\n","      <th>2.2 aggressive and emotive attacks</th>\n","      <td>673</td>\n","    </tr>\n","    <tr>\n","      <th>2.3 dehumanising attacks &amp; overt sexual objectification</th>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>3.1 casual use of gendered slurs, profanities, and insults</th>\n","      <td>637</td>\n","    </tr>\n","    <tr>\n","      <th>3.2 immutable gender differences and gender stereotypes</th>\n","      <td>417</td>\n","    </tr>\n","    <tr>\n","      <th>3.3 backhanded gendered compliments</th>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>3.4 condescending explanations or unwelcome advice</th>\n","      <td>47</td>\n","    </tr>\n","    <tr>\n","      <th>4.1 supporting mistreatment of individual women</th>\n","      <td>75</td>\n","    </tr>\n","    <tr>\n","      <th>4.2 supporting systemic discrimination against women as a group</th>\n","      <td>258</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dfe0780-00d8-41e4-8dfa-c4a900c2412f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8dfe0780-00d8-41e4-8dfa-c4a900c2412f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8dfe0780-00d8-41e4-8dfa-c4a900c2412f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["data[[\"text\", label_column]].groupby(label_column).count()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675721169718,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"p2LlvVnt-Z-Q"},"outputs":[],"source":["# label_values = data[label_column].unique()\n","# label_values\n","if label_column == \"label_sexist\":\n","  label_values = [\n","      'sexist',\n","      'not sexist',\n","  ]\n","\n","  label_map = {\n","      0: 'not sexist',\n","      1: 'sexist', \n","      'not sexist':0,\n","      'sexist':1,\n","  }\n","elif label_column == \"label_category\":\n","  label_values = [\n","      '1. threats, plans to harm and incitement',\n","      '2. derogation',\n","      '3. animosity',\n","      '4. prejudiced discussions',\n","  ]\n","\n","  label_map = {\n","      0: '1. threats, plans to harm and incitement',\n","      1: '2. derogation', \n","      2: '3. animosity', \n","      3: '4. prejudiced discussions',\n","      '1. threats, plans to harm and incitement':0,\n","      '2. derogation':1,\n","      '3. animosity':2,\n","      '4. prejudiced discussions':3,\n","  }\n","elif label_column == \"label_vector\":\n","  label_values = [\n","      '1.1 threats of harm',\n","      '1.2 incitement and encouragement of harm',\n","      '2.1 descriptive attacks',\n","      '2.2 aggressive and emotive attacks',\n","      '2.3 dehumanising attacks & overt sexual objectification',\n","      '3.1 casual use of gendered slurs, profanities, and insults',\n","      '3.2 immutable gender differences and gender stereotypes',\n","      '3.3 backhanded gendered compliments',\n","      '3.4 condescending explanations or unwelcome advice',\n","      '4.1 supporting mistreatment of individual women',\n","      '4.2 supporting systemic discrimination against women as a group',\n","  ]\n","\n","  label_map = {\n","      0: '1.1 threats of harm',\n","      1: '1.2 incitement and encouragement of harm',\n","      2: '2.1 descriptive attacks',\n","      3: '2.2 aggressive and emotive attacks',\n","      4: '2.3 dehumanising attacks & overt sexual objectification',\n","      5: '3.1 casual use of gendered slurs, profanities, and insults',\n","      6: '3.2 immutable gender differences and gender stereotypes',\n","      7: '3.3 backhanded gendered compliments',\n","      8: '3.4 condescending explanations or unwelcome advice',\n","      9: '4.1 supporting mistreatment of individual women',\n","      10: '4.2 supporting systemic discrimination against women as a group',\n","      '1.1 threats of harm': 0,\n","      '1.2 incitement and encouragement of harm': 1,\n","      '2.1 descriptive attacks': 2,\n","      '2.2 aggressive and emotive attacks': 3,\n","      '2.3 dehumanising attacks & overt sexual objectification': 4,\n","      '3.1 casual use of gendered slurs, profanities, and insults': 5,\n","      '3.2 immutable gender differences and gender stereotypes': 6,\n","      '3.3 backhanded gendered compliments': 7,\n","      '3.4 condescending explanations or unwelcome advice': 8,\n","      '4.1 supporting mistreatment of individual women': 9,\n","      '4.2 supporting systemic discrimination against women as a group': 10,\n","  }\n","else:\n","  raise Exception(\"Unknown label column\")"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675721169719,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"kEGNfg3Pqqcu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675721169719,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"F_7dGuv-qqfV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675721169719,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"dFUQTISpqqkn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1675721169719,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"AC3W2eaNqqm4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675721169720,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"LX_X0J51qqpt"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675721169720,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"fW5xyG34qqx7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675721169720,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"-BwuwwuY-jIN"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hBi-JWdA-ByY"},"source":["# Prepare Data"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12807,"status":"ok","timestamp":1675721182511,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"4nS99zjk-HtQ","outputId":"4b36e9e2-5105-4fc0-dca4-abe5bf10b6a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2405,"status":"ok","timestamp":1675721184911,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"EOTjJ2im-EOV"},"outputs":[],"source":["import transformers"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["524dd8ecb516460ebc7652f40b4e9f59","5eea1111c8274d5d849afcfb474299cc","27fcb225de224fe79218975b71d896bf","30453575d4fe4a349549e22acee6537e","c09082f4b0c64f93888e305d6f181d56","1b2d65c3adbf4377b85fd5ebb472b7af","9ee0f558c78940deb09b7c319ed90694","9b6fec6b616a46289394b26875b1ef4c","dcdcbf8ec06c434fb3c4af0f094fd568","7502fc1520ac49cc9ab592a80e114d16","a11c20c1eed1407f8ba9e6983e7cc762","ff5749f2feae4a139230b644b80b89dd","5571317b023c4fc08b40353417c67f92","9521ec2dbfa644b599d5be83a12d3637","1d4b4da8f6e5496fa98ba4abb776a82d","f17dc43dcf314f7fa571566cf05dd295","395833994453467db9f9157ea0d1fcd1","5755d0248d874c53afb6c61b619e12e1","1e22750df9034b6da13df37b9acbce9f","99eaa965cb3e4a999ac8fc4b43266fb6","2dd201062a594046926dc95228e206d3","1dbb8b2db754442793d557b3d076b366","e5b5c1d87cb045d58075c38693cc424f","96ab81c2bde1407783f579f29cc6d0ae","41685c59342d447e93522ff4fa9a1080","f4b3646d55c14a829b6b3d442dbfc3a9","4ffb01b0ef354da6bfe52026476090a2","8db642e39e8b46dabcb8124ad3812dac","092bed07e1734398ad17f94ad00fcedf","f0e203dae58442acb83d2bdf9665bef9","439341e61b17478ea64febeed980b905","bf48e0801a7a4d7880b8266710cca75a","d48b0d9c6bf7462096d61cb926a23493"]},"executionInfo":{"elapsed":8767,"status":"ok","timestamp":1675721193676,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"sBeN6Q3C-FT8","outputId":"44e7a4b7-44ee-431b-e2ec-c2d95a12a57e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"524dd8ecb516460ebc7652f40b4e9f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff5749f2feae4a139230b644b80b89dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5b5c1d87cb045d58075c38693cc424f"}},"metadata":{}}],"source":["from transformers import BertTokenizer\n","from transformers import AutoModel, AutoTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n","\n","# print('Original Text: ', texts[0], '\\n')\n","# print('Tokenized Text: ', tokenizer.tokenize(texts[0]), '\\n')\n","# print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[0])))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1675721193677,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"qhw5Fi-jmSDX"},"outputs":[],"source":["# !pip3 install -q nltk emoji==0.6.0"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1675721193677,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"vdtLMFoTmPPO"},"outputs":[],"source":["# from emoji import demojize\n","# from nltk.tokenize import TweetTokenizer\n","\n","\n","# nltktokenizer = TweetTokenizer()\n","\n","\n","# def normalizeToken(token):\n","#     lowercased_token = token.lower()\n","#     if token.startswith(\"@\"):\n","#         return \"@USER\"\n","#     elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n","#         return \"HTTPURL\"\n","#     elif len(token) == 1:\n","#         return demojize(token)\n","#     else:\n","#         if token == \"’\":\n","#             return \"'\"\n","#         elif token == \"…\":\n","#             return \"...\"\n","#         else:\n","#             return token\n","\n","\n","# def normalizeTweet(tweet):\n","#     tokens = nltktokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n","#     normTweet = \" \".join([normalizeToken(token) for token in tokens])\n","\n","#     normTweet = (\n","#         normTweet.replace(\"cannot \", \"can not \")\n","#         .replace(\"n't \", \" n't \")\n","#         .replace(\"n 't \", \" n't \")\n","#         .replace(\"ca n't\", \"can't\")\n","#         .replace(\"ai n't\", \"ain't\")\n","#     )\n","#     normTweet = (\n","#         normTweet.replace(\"'m \", \" 'm \")\n","#         .replace(\"'re \", \" 're \")\n","#         .replace(\"'s \", \" 's \")\n","#         .replace(\"'ll \", \" 'll \")\n","#         .replace(\"'d \", \" 'd \")\n","#         .replace(\"'ve \", \" 've \")\n","#     )\n","#     normTweet = (\n","#         normTweet.replace(\" p . m .\", \"  p.m.\")\n","#         .replace(\" p . m \", \" p.m \")\n","#         .replace(\" a . m .\", \" a.m.\")\n","#         .replace(\" a . m \", \" a.m \")\n","#     )\n","\n","#     return \" \".join(normTweet.split())"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1675721193677,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"Ar5fbAV_efCT"},"outputs":[],"source":["# normalizeTweet(\"DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier 😢\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3374,"status":"ok","timestamp":1675721197047,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"L6UVEaLG_DRs","outputId":"2aae2d66-1974-4cbd-d65f-582bafbf9aff"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1675721197048,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"GBbtE1-DrIRU"},"outputs":[],"source":["from nltk.tokenize import  word_tokenize"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5327,"status":"ok","timestamp":1675721202372,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"6wWMWrsY_Ta0"},"outputs":[],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from nltk.tokenize import TweetTokenizer\n","nltktokenizer = TweetTokenizer()\n","\n","def word_tokenize(sent):\n","  sent = sent.lower()\n","  words = nltktokenizer.tokenize(sent)\n","  return words\n","\n","def label_to_number(label_map, label):\n","  return label_map[label]\n","\n","def get_tensor(df, lexicons):\n","  texts = df['text'].values\n","  labels = [label_to_number(label_map, d) for d in df[label_column].values]\n","\n","  text_ids = [tokenizer.encode(text, max_length=100, padding='max_length', truncation=True) for text in texts]\n","  text_ids_lengths = [len(text_ids[i]) for i in range(len(text_ids))]\n","\n","  att_masks = []\n","  for ids in text_ids:\n","      masks = [int(id > 0) for id in ids]\n","      att_masks.append(masks)\n","\n","  lexcon_emb = []\n","  for text in texts:\n","    words = word_tokenize(text)\n","\n","    embs = []\n","    for w in words:\n","      e = []\n","      for label in lexicons:\n","        if w in lexicons[label]:\n","          e.append(1.0)\n","        else:\n","          e.append(0.0)\n","      embs.append(e)\n","\n","    while len(embs) < 100:\n","      embs.append([0 for l in lexicons])\n","    lexcon_emb.append(embs)\n","\n","  tensor_x = torch.tensor(text_ids)\n","  tensor_y = torch.tensor(labels)\n","  tensor_m = torch.tensor(att_masks)\n","  tensor_l = torch.tensor(lexcon_emb)\n","\n","  return tensor_x, tensor_y, tensor_m, tensor_l\n","\n","\n","def get_loader(train, test, val, lexicons, batch_size=32):\n","  \n","  train_x, train_y, train_m, train_l = get_tensor(train, lexicons)\n","  test_x, test_y, test_m, test_l = get_tensor(test, lexicons)\n","  val_x, val_y, val_m, val_l = get_tensor(train, lexicons)\n","\n","  train_data = TensorDataset(train_x, train_m, train_y, train_l)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","  val_data = TensorDataset(val_x, val_m, val_y, val_l)\n","  val_sampler = SequentialSampler(val_data)\n","  val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","\n","  test_data = TensorDataset(test_x, test_m, test_y, test_l)\n","  test_sampler = SequentialSampler(test_data)\n","  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","\n","  return train_dataloader, test_dataloader, val_dataloader"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1675721202372,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"TA2CQ3S0BMut"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1YjHFpnFCPbH"},"source":["## Load Model"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":379,"status":"ok","timestamp":1675721202749,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"dtNgYRKVhRHp"},"outputs":[],"source":["from transformers import BertForSequenceClassification\n","from transformers import BertPreTrainedModel, BertModel\n","from typing import List, Optional, Tuple, Union\n","\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","\n","class CustomBertForSequenceClassification(BertForSequenceClassification):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.config = config\n","\n","        self.bert = BertModel(config)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(in_features=config.hidden_size+config.num_labels, out_features=config.num_labels)\n","        self.lstm = nn.LSTM(input_size=config.num_labels, hidden_size=config.num_labels, num_layers=1, batch_first=True)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        token_type_ids: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.Tensor] = None,\n","        head_mask: Optional[torch.Tensor] = None,\n","        inputs_embeds: Optional[torch.Tensor] = None,\n","        labels: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","        lexicon_emb: Optional[torch.Tensor] = None\n","    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n","\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","\n","        lstm_output, (ht, ct) = self.lstm(lexicon_emb)\n","        ht = torch.squeeze(ht, dim=0)\n","        \n","        concated_ouput = torch.cat((pooled_output, ht), 1)\n","        \n","        logits = self.classifier(concated_ouput)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1675721202749,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"ImC6UK8IxgXC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1675721202750,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"oGWuuWQvCQUr"},"outputs":[],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import AutoModel, AutoTokenizer\n","\n","# num_labels = len(set(labels))\n","\n","# model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels, output_attentions=False, output_hidden_states=False)\n","# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels, output_attentions=False, output_hidden_states=False)\n","# model = BertForSequenceClassification.from_pretrained(\"vinai/bertweet-large\", num_labels=num_labels, output_attentions=False, output_hidden_states=False)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1675721204206,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"rgtlRHRaCVJN"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# model = model.to(device)\n","\n","# print(device)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1675721204206,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"ZAEijbXfCcwG"},"outputs":[],"source":["# def count_parameters(model):\n","#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# print('Number of trainable parameters:', count_parameters(model), '\\n', model)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675721204207,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"3q1lUg7QCgNb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"th14wYEDCnmP"},"source":["## Fine-tune"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675721204207,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"rK6fvY2VCpHc"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","import time\n","import numpy as np\n","import random\n","from tqdm.notebook import tqdm\n","import os\n","\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\n","def run(train_dataloader, val_dataloader, model, optimizer, scheduler, num_epochs=5, add_lexicon=True, tmp_out_dir=\"./Models/_tmp\"):\n","  train_losses = []\n","  val_losses = []\n","  num_mb_train = len(train_dataloader)\n","  num_mb_val = len(val_dataloader)\n","\n","  if num_mb_val == 0:\n","      num_mb_val = 1\n","\n","  best_loss = None\n","\n","  for n in range(num_epochs):\n","      train_loss = 0\n","      val_loss = 0\n","      start_time = time.time()\n","      \n","      for k, (mb_x, mb_m, mb_y, mb_l) in enumerate(train_dataloader):\n","          optimizer.zero_grad()\n","          model.train()\n","          \n","          mb_x = mb_x.to(device)\n","          mb_m = mb_m.to(device)\n","          mb_y = mb_y.to(device)\n","          mb_l = mb_l.to(device)\n","          \n","          if add_lexicon:\n","            outputs = model(mb_x, attention_mask=mb_m, labels=mb_y, lexicon_emb=mb_l)\n","          else:\n","            outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n","          \n","          loss = outputs[0]\n","          #loss = model_loss(outputs[1], mb_y)\n","          loss.backward()\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","          optimizer.step()\n","          scheduler.step()\n","          \n","          train_loss += loss.data / num_mb_train\n","      \n","      print (\"\\nTrain loss after itaration %i: %f\" % (n+1, train_loss))\n","      train_losses.append(train_loss.cpu())\n","      \n","      with torch.no_grad():\n","          model.eval()\n","          \n","          for k, (mb_x, mb_m, mb_y, mb_l) in enumerate(val_dataloader):\n","              mb_x = mb_x.to(device)\n","              mb_m = mb_m.to(device)\n","              mb_y = mb_y.to(device)\n","              mb_l = mb_l.to(device)\n","          \n","              if add_lexicon:\n","                outputs = model(mb_x, attention_mask=mb_m, labels=mb_y, lexicon_emb=mb_l)\n","              else:\n","                outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n","              \n","              loss = outputs[0]\n","              #loss = model_loss(outputs[1], mb_y)\n","              \n","              val_loss += loss.data / num_mb_val\n","\n","          if best_loss is None or best_loss >= val_loss:\n","              \n","              if not os.path.exists(tmp_out_dir):\n","                  os.makedirs(tmp_out_dir)\n","                  \n","              model.save_pretrained(tmp_out_dir)\n","              # tokenizer.save_pretrained(tmp_out_dir)\n","\n","              best_loss = val_loss\n","\n","          print(\"Validation loss after itaration %i: %f\" % (n+1, val_loss))\n","          val_losses.append(val_loss.cpu())\n","      \n","      end_time = time.time()\n","      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","      print(f'Time: {epoch_mins}m {epoch_secs}s')\n","\n","  if add_lexicon:\n","    model = CustomBertForSequenceClassification.from_pretrained(tmp_out_dir)\n","  else:\n","    model = BertForSequenceClassification.from_pretrained(tmp_out_dir)\n","\n","  model = model.to(device)\n","  return model, train_losses, val_losses\n","\n","def fine_tune(train_dataloader, val_dataloader, seed_val = 42, num_epochs=5, add_lexicon=True, tmp_out_dir=\"./Models/_tmp\"):\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)\n","\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","  num_labels = len(label_values)\n","  if add_lexicon:\n","    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels, output_attentions=False, output_hidden_states=False)\n","  else:\n","    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels, output_attentions=False, output_hidden_states=False)\n","\n","  model = model.to(device)\n","\n","  learning_rate = 1e-5\n","  adam_epsilon = 1e-8\n","\n","  no_decay = ['bias', 'LayerNorm.weight']\n","  optimizer_grouped_parameters = [\n","      {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","      'weight_decay_rate': 0.2},\n","      {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","      'weight_decay_rate': 0.0}\n","  ]\n","\n","  optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n","\n","  total_steps = len(train_dataloader) * num_epochs\n","  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","  return run(train_dataloader, val_dataloader, model, optimizer, scheduler, num_epochs, add_lexicon, tmp_out_dir)\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1675721204207,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"ZERwPHvivvLs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675721204208,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"quH_lgeJCqKb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675721204208,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"QlbEs7kVCxj1"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","def get_prediction(model, test_dataloader, add_lexicon = True):\n","  outputs = []\n","  test_y = []\n","  with torch.no_grad():\n","      model.eval()\n","      for k, (mb_x, mb_m, mb_y, mb_l) in enumerate(test_dataloader):\n","          mb_x = mb_x.to(device)\n","          mb_m = mb_m.to(device)\n","          mb_l = mb_l.to(device)\n","          if add_lexicon:\n","            output = model(mb_x, attention_mask=mb_m, lexicon_emb=mb_l)\n","          else:\n","            output = model(mb_x, attention_mask=mb_m)\n","          outputs.append(output[0].to('cpu'))\n","          test_y.append(mb_y)\n","\n","  outputs = torch.cat(outputs)\n","\n","\n","  _, predicted_values = torch.max(outputs, 1)\n","  predicted_values = predicted_values.numpy()\n","  true_values = torch.concat(test_y).numpy()\n","\n","  test_accuracy = np.sum(predicted_values == true_values) / len(true_values)\n","\n","  # print(classification_report(true_values, predicted_values, target_names=[str(l) for l in label_values]))\n","  p, r, f1, sup = precision_recall_fscore_support(true_values, predicted_values, average=\"macro\")\n","\n","  return f1, p, r"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1675721204208,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"3MqO7DfJ0Ib9"},"outputs":[],"source":["# !pip install numba"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":2260,"status":"ok","timestamp":1675721206462,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"E4kLOTJEzZcK"},"outputs":[],"source":["from numba import cuda\n","\n","def train_with_lexicons(i, lex_path=None, num_epochs=5, add_lexicon=True):\n","  \n","  # d = cuda.get_current_device() \n","  # d.reset()\n","\n","  train = pd.read_csv(f\"Data/{i}_train.csv\")\n","  test = pd.read_csv(f\"Data/{i}_test.csv\")\n","  val = pd.read_csv(f\"Data/{i}_val.csv\")\n","\n","  if lex_path is None:\n","    lexicons = {}\n","    for l in label_values:\n","      lexicons[l] = {}\n","  else:\n","    with open(f'{lex_path}_{i}.json') as fin:\n","      lexicons = json.load(fin)\n","\n","  train_dataloader, test_dataloader, val_dataloader = get_loader(train,test, val, lexicons, batch_size=32)\n","  model, train_losses, val_losses = fine_tune(train_dataloader, val_dataloader, num_epochs=num_epochs, add_lexicon=add_lexicon, tmp_out_dir=\"/content/Models/_tmp\")\n","  f1, p, r = get_prediction(model, test_dataloader, add_lexicon=add_lexicon)\n","  print(f1, p, r)\n","  # all_f1.append(f1)\n","  # all_p.append(p)\n","  # all_r.append(r)\n","  \n","  # print(f\"F1:{np.mean(all_f1):.3f}±{np.std(all_f1):.3f}\")\n","  # print(f\"P:{np.mean(all_p):.3f}±{np.std(all_p):.3f}, R: {np.mean(all_r):.3f}±{np.std(all_r):.3f}\")\n","\n","# train_with_lexicons(\"Results/TaskB/lexicon_pmi_train\")"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1675721206462,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"_xEQSSCB5jyD","outputId":"4db4d5cc-368c-447c-e211-dd5fb6192ba5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{},"execution_count":30}],"source":["len(label_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":891,"referenced_widgets":["62cf359640844068a08f63e3ddb9a4dc","f4a7d9cf54344d4da5aa5f3d400e2f35","28e37895d6674b01be8ea38249d9a372","7f2854df17d54135b7d1c2ba9f7e5e15","b73d29dd1a684e12a04063dd5729a359","8548291aa4814b27be3a4a27ba191677","121e92eabebb40aca397e08d61778c6f","a86cfa784ff8482ebf823ba8ac6d8d3b","3b392d8f44eb4f4b9ec5f90100a767a0","7e4d78c46b68459580ed012da0f5c0b9","7c34ffbf1a364ef4b576df21ae4e37b0"]},"executionInfo":{"elapsed":687515,"status":"ok","timestamp":1675707282310,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"9uUA3gbZ0GB0","outputId":"fe18bed2-42e2-47e6-e17d-92734272921b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62cf359640844068a08f63e3ddb9a4dc","version_major":2,"version_minor":0},"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss after itaration 1: 1.917481\n","Time: 1m 2s\n","\n","Train loss after itaration 2: 1.836215\n","Validation loss after itaration 2: 1.663026\n","Time: 1m 4s\n","\n","Train loss after itaration 3: 1.647827\n","Validation loss after itaration 3: 1.477865\n","Time: 1m 5s\n","\n","Train loss after itaration 4: 1.497896\n","Validation loss after itaration 4: 1.328272\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.367554\n","Validation loss after itaration 5: 1.194982\n","Time: 1m 5s\n","\n","Train loss after itaration 6: 1.255318\n","Validation loss after itaration 6: 1.087991\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.161155\n","Validation loss after itaration 7: 1.010761\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.093961\n","Validation loss after itaration 8: 0.954237\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.041607\n","Validation loss after itaration 9: 0.911884\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.015033\n","Validation loss after itaration 10: 0.899699\n","Time: 1m 6s\n","0.2666314723730992 0.25441497298581167 0.282760913347428\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(0, None, num_epochs=10, add_lexicon=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":677277,"status":"ok","timestamp":1675707959563,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"Zxidlj4vbwT3","outputId":"833f4264-b3a2-4199-b6f0-cb18779956bb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.151875\n","Validation loss after itaration 1: 1.912476\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.858194\n","Validation loss after itaration 2: 1.691642\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.683404\n","Validation loss after itaration 3: 1.531013\n","Time: 1m 5s\n","\n","Train loss after itaration 4: 1.542826\n","Validation loss after itaration 4: 1.384490\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.415029\n","Validation loss after itaration 5: 1.273537\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.311321\n","Validation loss after itaration 6: 1.156685\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.234364\n","Validation loss after itaration 7: 1.082903\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.156497\n","Validation loss after itaration 8: 1.018582\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.114412\n","Validation loss after itaration 9: 0.982529\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.077555\n","Validation loss after itaration 10: 0.969965\n","Time: 1m 6s\n","0.2844908773075136 0.2726280682079694 0.31706181478706014\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(1, None, num_epochs=10, add_lexicon=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":675219,"status":"ok","timestamp":1675708634759,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"H55D5o59bwW2","outputId":"2f2cff0c-0e10-435e-a130-ca09e6a1380e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.171356\n","Validation loss after itaration 1: 1.927312\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.884149\n","Validation loss after itaration 2: 1.738191\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.710055\n","Validation loss after itaration 3: 1.561217\n","Time: 1m 5s\n","\n","Train loss after itaration 4: 1.569709\n","Validation loss after itaration 4: 1.413842\n","Time: 1m 5s\n","\n","Train loss after itaration 5: 1.453597\n","Validation loss after itaration 5: 1.297794\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.351510\n","Validation loss after itaration 6: 1.187304\n","Time: 1m 5s\n","\n","Train loss after itaration 7: 1.268589\n","Validation loss after itaration 7: 1.118750\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.196499\n","Validation loss after itaration 8: 1.054946\n","Time: 1m 5s\n","\n","Train loss after itaration 9: 1.149736\n","Validation loss after itaration 9: 1.018958\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.124279\n","Validation loss after itaration 10: 1.007056\n","Time: 1m 5s\n","0.27263911726469037 0.3426630044657889 0.2819405987332051\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(2, None, num_epochs=10, add_lexicon=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":677945,"status":"ok","timestamp":1675709312680,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"q3d32vEjbwaL","outputId":"b0bbdf85-9238-4899-c9da-0db37018b3d7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.166478\n","Validation loss after itaration 1: 1.930033\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.866752\n","Validation loss after itaration 2: 1.705876\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.694839\n","Validation loss after itaration 3: 1.544640\n","Time: 1m 5s\n","\n","Train loss after itaration 4: 1.555184\n","Validation loss after itaration 4: 1.403747\n","Time: 1m 5s\n","\n","Train loss after itaration 5: 1.436179\n","Validation loss after itaration 5: 1.280366\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.323519\n","Validation loss after itaration 6: 1.173385\n","Time: 1m 5s\n","\n","Train loss after itaration 7: 1.242063\n","Validation loss after itaration 7: 1.093836\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.176654\n","Validation loss after itaration 8: 1.038088\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.135285\n","Validation loss after itaration 9: 1.000688\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.108226\n","Validation loss after itaration 10: 0.992828\n","Time: 1m 7s\n","0.2717146698991825 0.26244136103390053 0.28373878373878375\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(3, None, num_epochs=10, add_lexicon=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":675815,"status":"ok","timestamp":1675709988477,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"ksa8FHh_b0FQ","outputId":"a18cec64-7ba6-473f-8ff1-396d23238559"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.165822\n","Validation loss after itaration 1: 1.934723\n","Time: 1m 7s\n","\n","Train loss after itaration 2: 1.868163\n","Validation loss after itaration 2: 1.699260\n","Time: 1m 6s\n","\n","Train loss after itaration 3: 1.677426\n","Validation loss after itaration 3: 1.540306\n","Time: 1m 5s\n","\n","Train loss after itaration 4: 1.537951\n","Validation loss after itaration 4: 1.379946\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.423791\n","Validation loss after itaration 5: 1.270963\n","Time: 1m 5s\n","\n","Train loss after itaration 6: 1.315130\n","Validation loss after itaration 6: 1.168090\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.228378\n","Validation loss after itaration 7: 1.084222\n","Time: 1m 5s\n","\n","Train loss after itaration 8: 1.165064\n","Validation loss after itaration 8: 1.027837\n","Time: 1m 5s\n","\n","Train loss after itaration 9: 1.120584\n","Validation loss after itaration 9: 0.992836\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.089290\n","Validation loss after itaration 10: 0.981722\n","Time: 1m 5s\n","0.26662747626452254 0.2589247084529351 0.2868673244289008\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(4, None, num_epochs=10, add_lexicon=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SH2woyPrb0Iq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZj7nSPFb0L2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9N3pF3Nyb0PA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RibOwQeDb0Sp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sb-c8Qpt0GE9"},"outputs":[],"source":["# train_with_lexicons(\"Results/TaskC/lexicon_shapley_train\", num_epochs=5, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnuvcRhSDW3l"},"outputs":[],"source":["train_with_lexicons(0, \"Results/TaskC/lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfxG5oasDW-X"},"outputs":[],"source":["train_with_lexicons(1, \"Results/TaskC/lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbG0x5l-DXBN"},"outputs":[],"source":["train_with_lexicons(2, \"Results/TaskC/lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4hOxUoaDXEV"},"outputs":[],"source":["train_with_lexicons(3, \"Results/TaskC/lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OnzM1xTDXG_"},"outputs":[],"source":["train_with_lexicons(4, \"Results/TaskC/lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKlmMjUdDXJ5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRIaDZRMDXNO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlHMVRyM0GJ9"},"outputs":[],"source":["# train_with_lexicons(\"Results/TaskC/augmented_lexicon_shapley_train\", num_epochs=5, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_SIs324Dfwk"},"outputs":[],"source":["train_with_lexicons(0, \"Results/TaskC/augmented_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zYlMEU4aDf9m"},"outputs":[],"source":["train_with_lexicons(1, \"Results/TaskC/augmented_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XR5LyleXDgAf"},"outputs":[],"source":["train_with_lexicons(2, \"Results/TaskC/augmented_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"dmHKpWgkDiQO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675722520228,"user_tz":0,"elapsed":641744,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"}},"outputId":"f4f7fe02-b3c1-4ad7-8b6e-5562adaede20"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'lstm.bias_hh_l0', 'classifier.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Train loss after itaration 1: 2.113019\n","Validation loss after itaration 1: 1.916066\n","Time: 1m 3s\n","\n","Train loss after itaration 2: 1.868072\n","Validation loss after itaration 2: 1.720920\n","Time: 1m 2s\n","\n","Train loss after itaration 3: 1.722684\n","Validation loss after itaration 3: 1.583214\n","Time: 1m 2s\n","\n","Train loss after itaration 4: 1.593446\n","Validation loss after itaration 4: 1.452900\n","Time: 1m 2s\n","\n","Train loss after itaration 5: 1.491029\n","Validation loss after itaration 5: 1.340801\n","Time: 1m 2s\n","\n","Train loss after itaration 6: 1.382604\n","Validation loss after itaration 6: 1.246422\n","Time: 1m 2s\n","\n","Train loss after itaration 7: 1.309726\n","Validation loss after itaration 7: 1.166614\n","Time: 1m 2s\n","\n","Train loss after itaration 8: 1.233793\n","Validation loss after itaration 8: 1.113083\n","Time: 1m 2s\n","\n","Train loss after itaration 9: 1.192590\n","Validation loss after itaration 9: 1.078386\n","Time: 1m 2s\n","\n","Train loss after itaration 10: 1.165869\n","Validation loss after itaration 10: 1.066768\n","Time: 1m 2s\n","0.24701543667241754 0.25351048005042215 0.25145387645387646\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(3, \"Results/TaskC/augmented_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"WMSH2dBtDiTh","colab":{"base_uri":"https://localhost:8080/","height":926,"referenced_widgets":["f3c8997c7dc6490caf414d883aa466a1","547e05875edb4241af9773a00fd183ce","9bd42eaf9ccd4a969f36e242c38e6db3","fe24422b001144278e0a11d58add9d71","153f145310d44abc932de69ab81b26b2","65e774c52c004f83ae33bec0d3de8968","640e0226677b4f21b2148027f4365832","da0d294c000f4716ab958a661eb56a52","4aae836d22f04d1dae494ad7e3c926da","29887a9158dc46d6992103b6f1d8ca4e","85bfb484e4b540cf9e00b5cd9aa82e66"]},"executionInfo":{"status":"ok","timestamp":1675721878505,"user_tz":0,"elapsed":651069,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"}},"outputId":"0a8e926c-4eb5-4722-b13c-933e8c61ed81"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3c8997c7dc6490caf414d883aa466a1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'lstm.bias_hh_l0', 'classifier.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Train loss after itaration 1: 2.104811\n","Validation loss after itaration 1: 1.909382\n","Time: 0m 58s\n","\n","Train loss after itaration 2: 1.865251\n","Validation loss after itaration 2: 1.725304\n","Time: 1m 0s\n","\n","Train loss after itaration 3: 1.709854\n","Validation loss after itaration 3: 1.573952\n","Time: 1m 1s\n","\n","Train loss after itaration 4: 1.580456\n","Validation loss after itaration 4: 1.436925\n","Time: 1m 2s\n","\n","Train loss after itaration 5: 1.466616\n","Validation loss after itaration 5: 1.308494\n","Time: 1m 2s\n","\n","Train loss after itaration 6: 1.355834\n","Validation loss after itaration 6: 1.200461\n","Time: 1m 2s\n","\n","Train loss after itaration 7: 1.267822\n","Validation loss after itaration 7: 1.123478\n","Time: 1m 2s\n","\n","Train loss after itaration 8: 1.209195\n","Validation loss after itaration 8: 1.067558\n","Time: 1m 2s\n","\n","Train loss after itaration 9: 1.157644\n","Validation loss after itaration 9: 1.031111\n","Time: 1m 2s\n","\n","Train loss after itaration 10: 1.123063\n","Validation loss after itaration 10: 1.021130\n","Time: 1m 2s\n","0.2661079937123043 0.2679131825749231 0.2799249038986313\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(4, \"Results/TaskC/augmented_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","source":[],"metadata":{"id":"y-0O9ycA6nU3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E3AJbvxM6nYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678521,"status":"ok","timestamp":1675710667489,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"4NvMw1lClLBA","outputId":"8d96f45f-6d60-41f3-8f95-d8b161915ef6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.113147\n","Validation loss after itaration 1: 1.927983\n","Time: 1m 7s\n","\n","Train loss after itaration 2: 1.874028\n","Validation loss after itaration 2: 1.723373\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.709893\n","Validation loss after itaration 3: 1.556929\n","Time: 1m 6s\n","\n","Train loss after itaration 4: 1.562318\n","Validation loss after itaration 4: 1.417052\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.456077\n","Validation loss after itaration 5: 1.298454\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.352556\n","Validation loss after itaration 6: 1.214289\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.271388\n","Validation loss after itaration 7: 1.133374\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.206552\n","Validation loss after itaration 8: 1.067268\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.163014\n","Validation loss after itaration 9: 1.038644\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.133669\n","Validation loss after itaration 10: 1.023889\n","Time: 1m 6s\n","0.2910194216247244 0.3530622856933184 0.29608626129009896\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(0, \"Results/TaskC/gptj_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678267,"status":"ok","timestamp":1675711345733,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"BAMbkzwR_5ij","outputId":"7a677d30-f37a-482e-9632-51f22a1f596a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.107515\n","Validation loss after itaration 1: 1.924624\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.874633\n","Validation loss after itaration 2: 1.722925\n","Time: 1m 6s\n","\n","Train loss after itaration 3: 1.715198\n","Validation loss after itaration 3: 1.580092\n","Time: 1m 5s\n","\n","Train loss after itaration 4: 1.593178\n","Validation loss after itaration 4: 1.459363\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.489871\n","Validation loss after itaration 5: 1.352094\n","Time: 1m 5s\n","\n","Train loss after itaration 6: 1.406913\n","Validation loss after itaration 6: 1.259244\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.317703\n","Validation loss after itaration 7: 1.195678\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.255364\n","Validation loss after itaration 8: 1.126683\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.217317\n","Validation loss after itaration 9: 1.096077\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.187241\n","Validation loss after itaration 10: 1.080057\n","Time: 1m 6s\n","0.2724863749996464 0.27395763160738357 0.29210468777819365\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(1, \"Results/TaskC/gptj_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678895,"status":"ok","timestamp":1675712024606,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"peMiTOFd_5lo","outputId":"8f59d29d-6466-4f83-a7b8-9a5a41d3407c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.114439\n","Validation loss after itaration 1: 1.915813\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.864229\n","Validation loss after itaration 2: 1.733121\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.704266\n","Validation loss after itaration 3: 1.565658\n","Time: 1m 6s\n","\n","Train loss after itaration 4: 1.575523\n","Validation loss after itaration 4: 1.444577\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.470187\n","Validation loss after itaration 5: 1.331046\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.375550\n","Validation loss after itaration 6: 1.227536\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.279518\n","Validation loss after itaration 7: 1.147265\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.218986\n","Validation loss after itaration 8: 1.092752\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.169296\n","Validation loss after itaration 9: 1.053541\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.135330\n","Validation loss after itaration 10: 1.041967\n","Time: 1m 6s\n","0.2426093360341051 0.248576655024386 0.25865872160338566\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(2, \"Results/TaskC/gptj_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":676280,"status":"ok","timestamp":1675712700882,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"VwGHK3Gw_5oz","outputId":"b771f218-3324-4ecc-f1b1-bfe62aebc843"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.113023\n","Validation loss after itaration 1: 1.916292\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.867967\n","Validation loss after itaration 2: 1.720599\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.718802\n","Validation loss after itaration 3: 1.580127\n","Time: 1m 6s\n","\n","Train loss after itaration 4: 1.592459\n","Validation loss after itaration 4: 1.451408\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.492296\n","Validation loss after itaration 5: 1.342419\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.384340\n","Validation loss after itaration 6: 1.247992\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.311594\n","Validation loss after itaration 7: 1.167580\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.235229\n","Validation loss after itaration 8: 1.116984\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.194877\n","Validation loss after itaration 9: 1.081606\n","Time: 1m 5s\n","\n","Train loss after itaration 10: 1.167346\n","Validation loss after itaration 10: 1.069699\n","Time: 1m 6s\n","0.24710926273368347 0.2543241782372217 0.2516342516342516\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(3, \"Results/TaskC/gptj_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678097,"status":"ok","timestamp":1675713378975,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"nsgGiWay_5rp","outputId":"30176924-502c-467b-d7ad-4ee9ec914af9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.104814\n","Validation loss after itaration 1: 1.909401\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.865328\n","Validation loss after itaration 2: 1.732356\n","Time: 1m 6s\n","\n","Train loss after itaration 3: 1.713504\n","Validation loss after itaration 3: 1.574612\n","Time: 1m 6s\n","\n","Train loss after itaration 4: 1.584844\n","Validation loss after itaration 4: 1.440028\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.472675\n","Validation loss after itaration 5: 1.319181\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.363138\n","Validation loss after itaration 6: 1.208649\n","Time: 1m 5s\n","\n","Train loss after itaration 7: 1.275731\n","Validation loss after itaration 7: 1.129904\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.216276\n","Validation loss after itaration 8: 1.076574\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.165052\n","Validation loss after itaration 9: 1.039771\n","Time: 1m 5s\n","\n","Train loss after itaration 10: 1.130873\n","Validation loss after itaration 10: 1.029648\n","Time: 1m 5s\n","0.25628629850802426 0.26069983937630997 0.2687851815520124\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(4, \"Results/TaskC/gptj_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud4vOj4X_5vB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nW1bVRHb_5ye"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eje4s-LL_52l"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8jagS1I_559"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":679432,"status":"ok","timestamp":1675714058403,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"RUJCFKfE8jZx","outputId":"971db966-caed-4a0e-9f2e-4d06fb96c630"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.113108\n","Validation loss after itaration 1: 1.927711\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.875843\n","Validation loss after itaration 2: 1.734539\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.712646\n","Validation loss after itaration 3: 1.558644\n","Time: 1m 6s\n","\n","Train loss after itaration 4: 1.563813\n","Validation loss after itaration 4: 1.417282\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.455670\n","Validation loss after itaration 5: 1.299924\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.353403\n","Validation loss after itaration 6: 1.216075\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.272336\n","Validation loss after itaration 7: 1.135991\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.207853\n","Validation loss after itaration 8: 1.069227\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.165709\n","Validation loss after itaration 9: 1.040481\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.136083\n","Validation loss after itaration 10: 1.025958\n","Time: 1m 6s\n","0.2862667731654127 0.3484390191294437 0.2906754602005447\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(0, \"Results/TaskC/bertweet_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678093,"status":"ok","timestamp":1675714736489,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"},"user_tz":0},"id":"gBv5H2_fC3TB","outputId":"a3ffafb9-21c3-4444-f849-28a934320992"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.107507\n","Validation loss after itaration 1: 1.924462\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.873616\n","Validation loss after itaration 2: 1.725807\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.716877\n","Validation loss after itaration 3: 1.580915\n","Time: 1m 6s\n","\n","Train loss after itaration 4: 1.593450\n","Validation loss after itaration 4: 1.455829\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.489641\n","Validation loss after itaration 5: 1.352132\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.402788\n","Validation loss after itaration 6: 1.257216\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.315335\n","Validation loss after itaration 7: 1.193277\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.252192\n","Validation loss after itaration 8: 1.124791\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.214719\n","Validation loss after itaration 9: 1.095195\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.185881\n","Validation loss after itaration 10: 1.077938\n","Time: 1m 6s\n","0.2795523321387278 0.2769964213096971 0.29854544792100546\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(1, \"Results/TaskC/bertweet_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"zq37blrq31zC","outputId":"a98d4378-2a93-4ddc-9bb6-875743a5ae6f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.114611\n","Validation loss after itaration 1: 1.913831\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.866433\n","Validation loss after itaration 2: 1.737446\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.709640\n","Validation loss after itaration 3: 1.569248\n","Time: 1m 6s\n","\n","Train loss after itaration 4: 1.581630\n","Validation loss after itaration 4: 1.455308\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.476649\n","Validation loss after itaration 5: 1.335123\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.383972\n","Validation loss after itaration 6: 1.235978\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.287617\n","Validation loss after itaration 7: 1.156460\n","Time: 1m 5s\n","\n","Train loss after itaration 8: 1.227801\n","Validation loss after itaration 8: 1.101061\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.178737\n","Validation loss after itaration 9: 1.061904\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.144861\n","Validation loss after itaration 10: 1.051264\n","Time: 1m 5s\n","0.23323235196797407 0.24387724797888133 0.24693276639916958\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(2, \"Results/TaskC/bertweet_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UekrPEpk3118","outputId":"1d1da526-56d2-4276-d161-c16304b435f0"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.113024\n","Validation loss after itaration 1: 1.916321\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.868304\n","Validation loss after itaration 2: 1.720276\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.719084\n","Validation loss after itaration 3: 1.579281\n","Time: 1m 6s\n","\n","Train loss after itaration 4: 1.590912\n","Validation loss after itaration 4: 1.450215\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.488698\n","Validation loss after itaration 5: 1.338226\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.380920\n","Validation loss after itaration 6: 1.246224\n","Time: 1m 6s\n","\n","Train loss after itaration 7: 1.308679\n","Validation loss after itaration 7: 1.164931\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.232801\n","Validation loss after itaration 8: 1.112065\n","Time: 1m 6s\n","\n","Train loss after itaration 9: 1.192005\n","Validation loss after itaration 9: 1.077798\n","Time: 1m 5s\n","\n","Train loss after itaration 10: 1.165344\n","Validation loss after itaration 10: 1.065571\n","Time: 1m 6s\n","0.24963712550395317 0.2550101293439302 0.2549151299151299\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(3, \"Results/TaskC/bertweet_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_q2vewl8315n","outputId":"c5e977fb-1780-460b-a6c8-b300d1368f9b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'lstm.bias_hh_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Train loss after itaration 1: 2.104812\n","Validation loss after itaration 1: 1.909380\n","Time: 1m 6s\n","\n","Train loss after itaration 2: 1.865668\n","Validation loss after itaration 2: 1.727483\n","Time: 1m 5s\n","\n","Train loss after itaration 3: 1.709116\n","Validation loss after itaration 3: 1.572256\n","Time: 1m 5s\n","\n","Train loss after itaration 4: 1.579257\n","Validation loss after itaration 4: 1.436177\n","Time: 1m 6s\n","\n","Train loss after itaration 5: 1.465896\n","Validation loss after itaration 5: 1.307985\n","Time: 1m 6s\n","\n","Train loss after itaration 6: 1.354778\n","Validation loss after itaration 6: 1.200565\n","Time: 1m 5s\n","\n","Train loss after itaration 7: 1.266695\n","Validation loss after itaration 7: 1.122575\n","Time: 1m 6s\n","\n","Train loss after itaration 8: 1.207857\n","Validation loss after itaration 8: 1.066470\n","Time: 1m 5s\n","\n","Train loss after itaration 9: 1.156377\n","Validation loss after itaration 9: 1.029832\n","Time: 1m 6s\n","\n","Train loss after itaration 10: 1.121612\n","Validation loss after itaration 10: 1.020173\n","Time: 1m 6s\n","0.2624496646538313 0.2674370220422852 0.2745454387490512\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["train_with_lexicons(4, \"Results/TaskC/bertweet_lexicon_shapley_train\", num_epochs=10, add_lexicon=True)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"5jKkNJeug69J","executionInfo":{"status":"ok","timestamp":1675720953625,"user_tz":0,"elapsed":2,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"}}},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"_RdT5m9P6qpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nlXjMSmX6qse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = {\"None_train\": [\n","[0.2666314723730992, 0.25441497298581167, 0.282760913347428,],\n","[0.2844908773075136, 0.2726280682079694, 0.31706181478706014,],\n","[0.27263911726469037, 0.3426630044657889, 0.2819405987332051,],\n","[0.2717146698991825, 0.26244136103390053, 0.28373878373878375,],\n","[0.26662747626452254, 0.2589247084529351, 0.2868673244289008,],\n","]\n",",\n","\n","\"lexicon_shapley_train\": [\n","[0.28632686494585946, 0.34872676828122373, 0.2907249667382366,],\n","[0.2754581357138901, 0.2726235593036556, 0.29424029361585113,],\n","[0.236895428274869, 0.24405792100100543, 0.2514233241505969,],\n","[0.2489954383044023, 0.25745932066277394, 0.25289687789687787,],\n","[0.26953512400084595, 0.2732475888343519, 0.28267061497767576,],\n","]\n",",\n","\n","\n","\"augmented_lexicon_shapley_train\": [\n","[0.2866668611770293, 0.34907370216648564, 0.29081618634746276,],\n","[0.26816102756608595, 0.2663593500287049, 0.2859740932443876,],\n","[0.2408551450847834, 0.2485332124700008, 0.2553758933205574,],\n","[0.24701543667241754, 0.25351048005042215, 0.25145387645387646,],\n","[0.2661079937123043, 0.2679131825749231, 0.2799249038986313,],\n","]\n",",\n","\n","\n","\"gptj_lexicon_shapley_train\": [\n","[0.2910194216247244, 0.3530622856933184, 0.29608626129009896,],\n","[0.2724863749996464, 0.27395763160738357, 0.29210468777819365,],\n","[0.2426093360341051, 0.248576655024386, 0.25865872160338566,],\n","[0.24710926273368347, 0.2543241782372217, 0.2516342516342516,],\n","[0.25628629850802426, 0.26069983937630997, 0.2687851815520124,],\n","]\n",",\n","\n","\"bertweet_lexicon_shapley_train\": [\n","[0.2862667731654127, 0.3484390191294437, 0.2906754602005447,],\n","[0.2795523321387278, 0.2769964213096971, 0.29854544792100546,],\n","[0.23323235196797407, 0.24387724797888133, 0.24693276639916958,],\n","[0.24963712550395317, 0.2550101293439302, 0.2549151299151299,],\n","[0.2624496646538313, 0.2674370220422852, 0.2745454387490512,],\n","]\n","}"],"metadata":{"id":"Dt2j75L96qvl","executionInfo":{"status":"ok","timestamp":1675724046088,"user_tz":0,"elapsed":505,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","for exp in results:\n","  aug_all_f1 = []\n","  aug_all_p = []\n","  aug_all_r = []\n","  for f1, p, r in results[exp]:\n","    aug_all_f1.append(f1)\n","    aug_all_p.append(p)\n","    aug_all_r.append(r)\n","  \n","  print(exp)\n","  print(f\"F1:{np.mean(aug_all_f1):.3f} ± {np.std(aug_all_f1):.3f}\")\n","  print(f\"P:{np.mean(aug_all_p):.3f} ± {np.std(aug_all_p):.3f}\")\n","  print(f\"R:{np.mean(aug_all_r):.3f} ± {np.std(aug_all_r):.3f}\")\n","  print(\"\")\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DByc3Au96qye","executionInfo":{"status":"ok","timestamp":1675724046089,"user_tz":0,"elapsed":7,"user":{"displayName":"Pakawat Nakwijit","userId":"03267729789266935520"}},"outputId":"6177bf23-ed06-4d28-eba0-21d604c1ea6e"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["None_train\n","F1:0.272 ± 0.007\n","P:0.278 ± 0.033\n","R:0.290 ± 0.013\n","\n","lexicon_shapley_train\n","F1:0.263 ± 0.018\n","P:0.279 ± 0.036\n","R:0.274 ± 0.019\n","\n","augmented_lexicon_shapley_train\n","F1:0.262 ± 0.016\n","P:0.277 ± 0.037\n","R:0.273 ± 0.016\n","\n","gptj_lexicon_shapley_train\n","F1:0.262 ± 0.018\n","P:0.278 ± 0.038\n","R:0.273 ± 0.018\n","\n","bertweet_lexicon_shapley_train\n","F1:0.262 ± 0.019\n","P:0.278 ± 0.037\n","R:0.273 ± 0.020\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MYmlUt0d7IBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aOmgvRS67IDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fo5Xcs5f7IGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-H7Kjfbg7K9"},"source":["## Create Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ca7acZN6Q9xS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NK5zQ_7RDeq"},"outputs":[],"source":["# ls ../Data/starting_ki/train_all_tasks.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gRmdTmcg8oi"},"outputs":[],"source":["# dftrain = pd.read_csv(\"../Data/starting_ki/train_all_tasks.csv\")\n","# dfdev = pd.read_csv(\"../Data/dev_task_b_entries.csv\")\n","# dftest = pd.read_csv(\"../Data/test_task_b_entries.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhcjkWrnhA99"},"outputs":[],"source":["df = dftrain\n","texts = df['text'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRqf5eGohEiq"},"outputs":[],"source":["text_ids = [tokenizer.encode(text, max_length=100, padding='max_length') for text in texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXJ-nXvEhdVB"},"outputs":[],"source":["att_masks = []\n","for ids in text_ids:\n","    masks = [int(id > 0) for id in ids]\n","    att_masks.append(masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWpmOOj_RxRb"},"outputs":[],"source":["lexcon_emb = []\n","for text in texts:\n","  words = word_tokenize(text)\n","\n","  embs = []\n","  for w in words:\n","    e = []\n","    for label in lexicons:\n","      if w in lexicons[label]:\n","        e.append(1.0)\n","      else:\n","        e.append(0.0)\n","    embs.append(e)\n","\n","  while len(embs) < 100:\n","    embs.append([0 for l in lexicons])\n","  lexcon_emb.append(embs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGZawtIRhfjM"},"outputs":[],"source":["test_x = torch.tensor(text_ids);\n","test_m = torch.tensor(att_masks);\n","test_l = torch.tensor(lexcon_emb);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9eKxAQgSM-U"},"outputs":[],"source":["len(df), test_x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4mmqM60hh3c"},"outputs":[],"source":["from tqdm import tqdm\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 32\n","test_data = TensorDataset(test_x, test_m, test_l)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","\n","outputs = []\n","with torch.no_grad():\n","    model.eval()\n","    for k, (mb_x, mb_m, mb_l) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n","        mb_x = mb_x.to(device)\n","        mb_m = mb_m.to(device)\n","        mb_l = mb_l.to(device)\n","        output = model(mb_x, attention_mask=mb_m, lexicon_emb=mb_l)\n","        outputs.append(output[0].to('cpu'))\n","\n","outputs = torch.cat(outputs)\n","_, predicted_values = torch.max(outputs, 1)\n","predicted_values = predicted_values.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leZAWcW6h-N0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwIC2kqJhm6q"},"outputs":[],"source":["def number_to_label(label):\n","  return label_map[label]\n","\n","predicted_labels = list(map(number_to_label, predicted_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nH6v8LwuSJat"},"outputs":[],"source":["len(predicted_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ujj7PmjSGrL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueBXKUIchvu9"},"outputs":[],"source":["df[\"label_pred\"] = predicted_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nQTiCvbfIqA"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSgqgKL9hvJv"},"outputs":[],"source":["# df[[\"rewire_id\", \"label_pred\"]].to_csv(f\"../Results/1b_lexbert_test.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zE0lU4A6iRC0"},"outputs":[],"source":["df.to_csv(f\"../Results/1b_lexbert_train.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2j5aiDEiVFh"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"121e92eabebb40aca397e08d61778c6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28e37895d6674b01be8ea38249d9a372":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a86cfa784ff8482ebf823ba8ac6d8d3b","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b392d8f44eb4f4b9ec5f90100a767a0","value":440473133}},"3b392d8f44eb4f4b9ec5f90100a767a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62cf359640844068a08f63e3ddb9a4dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4a7d9cf54344d4da5aa5f3d400e2f35","IPY_MODEL_28e37895d6674b01be8ea38249d9a372","IPY_MODEL_7f2854df17d54135b7d1c2ba9f7e5e15"],"layout":"IPY_MODEL_b73d29dd1a684e12a04063dd5729a359"}},"7c34ffbf1a364ef4b576df21ae4e37b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e4d78c46b68459580ed012da0f5c0b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f2854df17d54135b7d1c2ba9f7e5e15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e4d78c46b68459580ed012da0f5c0b9","placeholder":"​","style":"IPY_MODEL_7c34ffbf1a364ef4b576df21ae4e37b0","value":" 440M/440M [00:10&lt;00:00, 39.6MB/s]"}},"8548291aa4814b27be3a4a27ba191677":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a86cfa784ff8482ebf823ba8ac6d8d3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b73d29dd1a684e12a04063dd5729a359":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4a7d9cf54344d4da5aa5f3d400e2f35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8548291aa4814b27be3a4a27ba191677","placeholder":"​","style":"IPY_MODEL_121e92eabebb40aca397e08d61778c6f","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"524dd8ecb516460ebc7652f40b4e9f59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5eea1111c8274d5d849afcfb474299cc","IPY_MODEL_27fcb225de224fe79218975b71d896bf","IPY_MODEL_30453575d4fe4a349549e22acee6537e"],"layout":"IPY_MODEL_c09082f4b0c64f93888e305d6f181d56"}},"5eea1111c8274d5d849afcfb474299cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b2d65c3adbf4377b85fd5ebb472b7af","placeholder":"​","style":"IPY_MODEL_9ee0f558c78940deb09b7c319ed90694","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"27fcb225de224fe79218975b71d896bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b6fec6b616a46289394b26875b1ef4c","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dcdcbf8ec06c434fb3c4af0f094fd568","value":231508}},"30453575d4fe4a349549e22acee6537e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7502fc1520ac49cc9ab592a80e114d16","placeholder":"​","style":"IPY_MODEL_a11c20c1eed1407f8ba9e6983e7cc762","value":" 232k/232k [00:00&lt;00:00, 263kB/s]"}},"c09082f4b0c64f93888e305d6f181d56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b2d65c3adbf4377b85fd5ebb472b7af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ee0f558c78940deb09b7c319ed90694":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b6fec6b616a46289394b26875b1ef4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcdcbf8ec06c434fb3c4af0f094fd568":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7502fc1520ac49cc9ab592a80e114d16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a11c20c1eed1407f8ba9e6983e7cc762":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff5749f2feae4a139230b644b80b89dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5571317b023c4fc08b40353417c67f92","IPY_MODEL_9521ec2dbfa644b599d5be83a12d3637","IPY_MODEL_1d4b4da8f6e5496fa98ba4abb776a82d"],"layout":"IPY_MODEL_f17dc43dcf314f7fa571566cf05dd295"}},"5571317b023c4fc08b40353417c67f92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_395833994453467db9f9157ea0d1fcd1","placeholder":"​","style":"IPY_MODEL_5755d0248d874c53afb6c61b619e12e1","value":"Downloading (…)okenizer_config.json: 100%"}},"9521ec2dbfa644b599d5be83a12d3637":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e22750df9034b6da13df37b9acbce9f","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99eaa965cb3e4a999ac8fc4b43266fb6","value":28}},"1d4b4da8f6e5496fa98ba4abb776a82d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dd201062a594046926dc95228e206d3","placeholder":"​","style":"IPY_MODEL_1dbb8b2db754442793d557b3d076b366","value":" 28.0/28.0 [00:00&lt;00:00, 459B/s]"}},"f17dc43dcf314f7fa571566cf05dd295":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"395833994453467db9f9157ea0d1fcd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5755d0248d874c53afb6c61b619e12e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e22750df9034b6da13df37b9acbce9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99eaa965cb3e4a999ac8fc4b43266fb6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2dd201062a594046926dc95228e206d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dbb8b2db754442793d557b3d076b366":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5b5c1d87cb045d58075c38693cc424f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96ab81c2bde1407783f579f29cc6d0ae","IPY_MODEL_41685c59342d447e93522ff4fa9a1080","IPY_MODEL_f4b3646d55c14a829b6b3d442dbfc3a9"],"layout":"IPY_MODEL_4ffb01b0ef354da6bfe52026476090a2"}},"96ab81c2bde1407783f579f29cc6d0ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8db642e39e8b46dabcb8124ad3812dac","placeholder":"​","style":"IPY_MODEL_092bed07e1734398ad17f94ad00fcedf","value":"Downloading (…)lve/main/config.json: 100%"}},"41685c59342d447e93522ff4fa9a1080":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0e203dae58442acb83d2bdf9665bef9","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_439341e61b17478ea64febeed980b905","value":570}},"f4b3646d55c14a829b6b3d442dbfc3a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf48e0801a7a4d7880b8266710cca75a","placeholder":"​","style":"IPY_MODEL_d48b0d9c6bf7462096d61cb926a23493","value":" 570/570 [00:00&lt;00:00, 8.62kB/s]"}},"4ffb01b0ef354da6bfe52026476090a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8db642e39e8b46dabcb8124ad3812dac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"092bed07e1734398ad17f94ad00fcedf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0e203dae58442acb83d2bdf9665bef9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"439341e61b17478ea64febeed980b905":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf48e0801a7a4d7880b8266710cca75a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d48b0d9c6bf7462096d61cb926a23493":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3c8997c7dc6490caf414d883aa466a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_547e05875edb4241af9773a00fd183ce","IPY_MODEL_9bd42eaf9ccd4a969f36e242c38e6db3","IPY_MODEL_fe24422b001144278e0a11d58add9d71"],"layout":"IPY_MODEL_153f145310d44abc932de69ab81b26b2"}},"547e05875edb4241af9773a00fd183ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65e774c52c004f83ae33bec0d3de8968","placeholder":"​","style":"IPY_MODEL_640e0226677b4f21b2148027f4365832","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"9bd42eaf9ccd4a969f36e242c38e6db3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da0d294c000f4716ab958a661eb56a52","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4aae836d22f04d1dae494ad7e3c926da","value":440473133}},"fe24422b001144278e0a11d58add9d71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29887a9158dc46d6992103b6f1d8ca4e","placeholder":"​","style":"IPY_MODEL_85bfb484e4b540cf9e00b5cd9aa82e66","value":" 440M/440M [00:02&lt;00:00, 168MB/s]"}},"153f145310d44abc932de69ab81b26b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65e774c52c004f83ae33bec0d3de8968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"640e0226677b4f21b2148027f4365832":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da0d294c000f4716ab958a661eb56a52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4aae836d22f04d1dae494ad7e3c926da":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29887a9158dc46d6992103b6f1d8ca4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85bfb484e4b540cf9e00b5cd9aa82e66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
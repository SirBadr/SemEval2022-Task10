{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc302fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "922802fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_lexicon = data_path + \"shapley_lexicon_1.txt\"\n",
    "derogation_lexicon = data_path + \"shapley_lexicon_2.txt\"\n",
    "animosity_lexicon = data_path + \"shapley_lexicon_3.txt\"\n",
    "predjudice_discussion_lexicon = data_path +  \"shapley_lexicon_4.txt\"\n",
    "\n",
    "threat_lexicon_aug = data_path + \"shapley_lexicon_1_aug.txt\"\n",
    "derogation_lexicon_aug = data_path + \"shapley_lexicon_2_aug.txt\"\n",
    "animosity_lexicon_aug = data_path + \"shapley_lexicon_3_aug.txt\"\n",
    "predjudice_discussion_lexicon_aug = data_path + \"shapley_lexicon_4_aug.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6501d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_words_map = {\n",
    "    \"Threat\":threat_words,\n",
    "    \"Derogation\":derogation_words,\n",
    "    \"Animosity\":animosity_words,\n",
    "    \"Prejudice discussion\":predjudice_discussion_words\n",
    "}\n",
    "\n",
    "label_aug_file_map = {\n",
    "    \"Threat\": threat_lexicon_aug,\n",
    "    \"Derogation\": derogation_lexicon_aug,\n",
    "    \"Animosity\": animosity_lexicon_aug,\n",
    "    \"Prejudice discussion\": predjudice_discussion_lexicon_aug\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "398b2bb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threat_words = []\n",
    "derogation_words = []\n",
    "animosity_words = []\n",
    "predjudice_discussion_words = []\n",
    "def get_lexicon_words_list():\n",
    "    # read threat words\n",
    "    f = open(threat_lexicon)\n",
    "    for word in f.read().split():\n",
    "        threat_words.append(word)\n",
    "        \n",
    "    # read derogation words\n",
    "    f = open(derogation_lexicon)\n",
    "    for word in f.read().split():\n",
    "        derogation_words.append(word)\n",
    "        \n",
    "    # read animosity words\n",
    "    f = open(animosity_lexicon)\n",
    "    for word in f.read().split():\n",
    "        animosity_words.append(word)\n",
    "        \n",
    "    # read prejudice discussion words\n",
    "    f = open(predjudice_discussion_lexicon)\n",
    "    for word in f.read().split():\n",
    "        predjudice_discussion_words.append(word)\n",
    "\n",
    "get_lexicon_words_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cafb7619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853\n",
      "4778\n",
      "4158\n",
      "2183\n"
     ]
    }
   ],
   "source": [
    "print(len(threat_words))\n",
    "print(len(derogation_words))\n",
    "print(len(animosity_words))\n",
    "print(len(predjudice_discussion_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da5eda73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(label_words_map.get('sdaw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1517e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_four_random_label_words(label):\n",
    "    if(label_words_map.get(label) != None):\n",
    "        res = random.choices(label_words_map[label], k=4)\n",
    "        return res[0],res[1],res[2],res[3]\n",
    "    else:\n",
    "        print(\"label not found\")\n",
    "        return None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8829f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_label():\n",
    "    return random.choices([\"Threat\", \"Derogation\", \"Animosity\", \"Prejudice discussion\"], k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3679d02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threat\n"
     ]
    }
   ],
   "source": [
    "print(get_random_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b4d010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ruined', 'full', '>', 'warfare')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_four_random_label_words(get_random_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82856123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(lst, filename):\n",
    "    # input text file\n",
    "    inputFile = filename\n",
    "\n",
    "    # Opening the given file in write mode\n",
    "    with open(inputFile, 'w') as filedata:\n",
    "\n",
    "        # Traverse in each element of the input list \n",
    "        for item in lst:\n",
    "\n",
    "            # Writing each element of the list into the file\n",
    "            # Here “%s\\n” % syntax is used to move to the next line after adding an item to the file.\n",
    "            filedata.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e66cf5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_list_to_file([\"test\", \"Hi\", \"Lol\"], \"test_write.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5e45884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_prompt(label, word1, word2, word3, word4):\n",
    "    # define a function that takes as input three samples and generates the prompt\n",
    "    # that we should pass to the GPT-3 language model for completion.\n",
    "    description = f\"Each item in the following list contains a {label} word\"\n",
    "    prompt = (f\"{description}\\n\"\n",
    "            f\"Item: {word1}.\\n\"\n",
    "            f\"Item: {word2}.\\n\"\n",
    "            f\"Item: {word3}.\\n\"\n",
    "            f\"Item: {word4}.\\n\"\n",
    "            f\"Item:\")\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "810b58f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each item in the following list contains a Threat word\n",
      "Item: talk.\n",
      "Item: woman.\n",
      "Item: burned.\n",
      "Item: bump.\n",
      "Item:\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = get_four_random_label_words(\"Threat\")\n",
    "print(get_label_prompt(\"Threat\", a,b,c,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad27a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TrainerCallback, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "max_input_len = 30\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "edbdc79b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m get_label_prompt(random_label, a, b, c, d)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# generate text using GPT-J model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     print('before_gentokens')\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     gen_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# block of code to generate Synthetic Tweets!\n",
    "# define the number of synthetic samples to generate\n",
    "n = 100\n",
    "new_texts = []\n",
    "# new_labels = []\n",
    "\n",
    "iter = 0\n",
    "while iter < n:\n",
    "    # get random label\n",
    "    random_label = get_random_label()\n",
    "    \n",
    "    # get four random words\n",
    "    a,b,c,d = get_four_random_label_words(random_label)\n",
    "    \n",
    "    # create the prompt\n",
    "    prompt = get_label_prompt(random_label, a, b, c, d)\n",
    "\n",
    "    # generate text using GPT-J model\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "#     print('before_gentokens')\n",
    "    gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=100,)\n",
    "#     print('after_gentokens')\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    \n",
    "    # the generated output will be in the form \"<text> (Sentiment: <label>)\"\n",
    "    data = gen_text.split('\\n')[3].strip('Item: ')\n",
    "    \n",
    "    if len(data) < 2:\n",
    "        # the format of the response is invalid\n",
    "        continue\n",
    "\n",
    "    text = data[0]\n",
    "    print('text', text);\n",
    "    label_words_map[random_label].append(text)\n",
    "    iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "696384ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "write_list_to_file(threat_words, label_aug_file_map[\"Threat\"]) # write list to the new file!\n",
    "write_list_to_file(derogation_words, label_aug_file_map[\"Derogation\"]) # write list to the new file!\n",
    "write_list_to_file(animosity_words, label_aug_file_map[\"Animosity\"]) # write list to the new file!\n",
    "write_list_to_file(predjudice_discussion_words, label_aug_file_map[\"Prejudice discussion\"]) # write list to the new file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b93c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
